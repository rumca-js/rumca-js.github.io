+++
title = "Banned on Reddit - for being (Mistakenly) a Bot"
date = 2025-06-14 08:43:32
draft = false
+++

Recently, something strange—and honestly a bit disheartening—happened to me. I was banned from the r/technology subreddit on Reddit. At first, I had no idea why. I hadn’t broken any obvious rules, hadn’t flamed anyone, or posted anything off-topic. I was just sharing my own open-source project and hobby repository, something I thought would be of genuine interest to others.

Shortly afterward, I received a notification from another subreddit, r/webscraping, stating that my actions appeared "suspicious." It turns out that Reddit moderators had flagged me as a bot. A bot. Me—a human being who just happens to love tinkering with code and sharing it online.

This raised an uncomfortable realization: moderators are increasingly relying on behavioral patterns to detect bots. They’re not checking if you're spamming, rude, or unhelpful. They’re watching how often you post, what you post, and how you phrase things. And with the proliferation of AI-generated content online, many communities are erring on the side of caution—flagging anything that feels "too consistent" or "too mechanical."

This has real consequences. In my case, I followed the subreddit rules. Self-promotion is allowed on many subreddits like r/technology and r/webscraping—as long as it’s relevant, constructive, and not spammy. Yet, that didn’t save me. The problem is, bots don’t always break the rules either. They just act… differently. And now, if you look like a bot, you're treated like one.

We’re entering an era where the signal-to-noise ratio is getting worse. There are now countless AI chatbots online posting in forums, blogs, and social media. While some contribute meaningfully, many are simply noise—regurgitating content, farming karma, or promoting spam. Communities are overwhelmed and, in response, moderation systems are tightening. But when you can no longer tell a human from a bot, you start banning both.

This trajectory is worrying. If it continues, we may start to see increased demands for verification: digital IDs, biometric scans, identity proofs. The internet may soon require you to prove you're human—again and again—just to participate. Every app, every login, every post might ask you to verify not just your identity, but your humanness.

Ironically, the more this happens, the more personal information we’ll be forced to share. Your age, your face, your behavioral patterns, your digital fingerprints—everything becomes part of an online profile. And where there’s more data, there’s more potential for abuse. The trade-off between trust and privacy will become steeper.

I’m just a hobbyist developer who wanted to share something I built. But this small incident reflects a much larger trend. As bots get better, and humans get harder to recognize, the online experience is going to change—for all of us.

And not necessarily for the better.

